{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video search in scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to create a Flask based REST API that can be used to query for scenes inside a video. First we define a few helper functions needed for this functionality.\n",
    "The first of these is the tf-idf functions needed for doing a plot-subtitle based search on the videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-Idf and cosine similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def term_frequency(document):               # output the normalized term frequencies for all words\n",
    "    term_frequency = dict()                 # creates a dictionary (hash table)\n",
    "    for word in document:\n",
    "        if word not in term_frequency:\n",
    "            term_frequency[word] = document.count(word)/float(len(document))\n",
    "    return term_frequency\n",
    "\n",
    "def inv_doc_frequency(documents):\n",
    "    allwords, idf, doc_word_set = set(), defaultdict(lambda: 0.0), []\n",
    "    print('Number of documents is ', len(documents))\n",
    "    for doc in documents:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            idf[word] += 1\n",
    "    for word in idf:\n",
    "        idf[word] = 1.0 + math.log(float(len(documents))/idf[word])\n",
    "    return idf\n",
    "\n",
    "def tf_idf(tf_list, idf):\n",
    "    tfIdf = dict()\n",
    "    for doc in tf_list:                               # create a dictionary of dictionaries\n",
    "        tfIdf[doc] = dict()\n",
    "        for term in tf_list[doc]:\n",
    "            tfIdf[doc][term] = tf_list[doc][term] * idf[term]\n",
    "    return tfIdf\n",
    "\n",
    "def cosine_similarity(query, idf, tfIdf, document_size):\n",
    "    query_set = set(query)                       # take the unique words in the query\n",
    "    print('query is ', query_set)\n",
    "    print('IDF is', idf)\n",
    "    if len(query_set) == 0:\n",
    "        return (-1, -1)\n",
    "    term_freq = 1.0/len(query_set)               # the frequency of each term of the query is the same\n",
    "    max_similarity = 0                           # initialization of similarity\n",
    "    for doc in range(document_size):\n",
    "        dotproductsum = 0                        # sum of the dot product of query and document\n",
    "        query_mag = 0                            # query magnitude and doc. term magnitude\n",
    "        doc_mag = 0\n",
    "        for term in query_set:                   # global idf * term_freq * tf_idf of term in document\n",
    "            dotproductsum += (idf[term] if term in idf else 0) * term_freq * (tfIdf[doc][term] if term in tfIdf[doc] else 0)\n",
    "            query_mag += math.pow((idf[term] if term in idf else 0) * term_freq, 2)\n",
    "        for word in tfIdf[doc]:                 # take the tf_idf of all terms in the document and square-add\n",
    "            doc_mag += math.pow(tfIdf[doc][word], 2)\n",
    "        cosine_sim = dotproductsum / (math.sqrt(query_mag * doc_mag) + 0.001) # add 0.001 to avoid 0/0 division\n",
    "        if (cosine_sim >= max_similarity):       # check the highest cosine similarity in each iteration\n",
    "             max_similarity = cosine_sim\n",
    "             max_doc = doc\n",
    "    if (max_similarity == 0):\n",
    "        max_doc = \"None\"\n",
    "    return (max_similarity, max_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot to shot utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shot_helper(plot_sentences, subtitle_data, stamp_data):\n",
    "    plot_sub_data = plot_sub_assigner(plot_sentences, subtitle_data['sub_text'])\n",
    "    plot_to_sub = plot_sub_data['plot_to_sub']\n",
    "    tf_list = plot_sub_data['tf_list']\n",
    "    idf, tfIdf = plot_sub_data['idf'], plot_sub_data['tfIdf']\n",
    "    sub_to_shot = sub_shot_assigner(subtitle_data['sub_stamps'], stamp_data['scene_stamps'])\n",
    "    plot_to_shot = plot_shot_assigner(plot_to_sub, sub_to_shot)\n",
    "    return {'plot_to_shot': plot_to_shot, 'idf': idf, 'tfIdf': tfIdf, 'tf_list': tf_list}\n",
    "\n",
    "def plot_sub_assigner(plot_sentences, sub_text):  # used by sim. function 1\n",
    "    # plot assignment to shots\n",
    "    plot_to_sub = [[] for i in range(len(plot_sentences))]\n",
    "    tf_list = dict()\n",
    "    # find term frequency for all plot sentences\n",
    "    for index, plot_sentence in enumerate(plot_sentences):\n",
    "        tf_list[index] = term_frequency(plot_sentence)\n",
    "    idf = inv_doc_frequency(plot_sentences)\n",
    "    tfIdf = tf_idf(tf_list, idf)\n",
    "    # which plot sentence most similar with subtitle?\n",
    "    for index, sub_sentence in enumerate(sub_text):\n",
    "        similarity = cosine_similarity(sub_sentence, idf, tfIdf, len(plot_sentences))\n",
    "        if similarity == (-1, -1) or similarity == (0, 'None'):    # query has a problem\n",
    "            continue\n",
    "        else:\n",
    "            plot_to_sub[similarity[1]].append((index, similarity[0]))\n",
    "    # sort plot_to_sub before return\n",
    "    for i in range(len(plot_to_sub)):\n",
    "        plot_to_sub[i] = sorted(plot_to_sub[i], key = lambda x: x[1], reverse=True)\n",
    "    return {'plot_to_sub': plot_to_sub, 'idf': idf, 'tfIdf': tfIdf, 'tf_list': tf_list}\n",
    "\n",
    "def sub_shot_assigner(sub_stamps, scene_stamps):\n",
    "    ''' first part assigns shot numbers to each part of a subtitle\n",
    "        optimizations can be done here '''\n",
    "    temp_sub_shot = [[0, 0] for i in range(len(sub_stamps))]\n",
    "    for sub_index, sub in enumerate(sub_stamps):\n",
    "        for scene_index, scene in enumerate(scene_stamps):\n",
    "            if (sub[0] < scene[1]):\n",
    "                temp_sub_shot[sub_index][0] = scene_index\n",
    "                break\n",
    "    for sub_index, sub in enumerate(sub_stamps):\n",
    "        for scene_index, scene in enumerate(scene_stamps):\n",
    "            if (sub[1] < scene[1]):\n",
    "                temp_sub_shot[sub_index][1] = scene_index\n",
    "                break\n",
    "    # second part assigns the subtitles properly to shots based on above information\n",
    "    sub_to_shot = [None]*len(temp_sub_shot)\n",
    "    for index, tup in enumerate(temp_sub_shot):\n",
    "        if (tup[0] == tup[1]):    # subtitle start and end in the same shot\n",
    "            sub_to_shot[index] = tup[0]\n",
    "        else:    # tup[1] - tup[0] >= 1\n",
    "            diff = tup[1] - tup[0]    # (scene gap between the subtitle start and end) + 1\n",
    "            for i in range(1, diff):    # if difference is 1 it won't work\n",
    "                sub_to_shot[index+i] = tup[0] + i\n",
    "            if ((scene_stamps[tup[0]][1]-sub_stamps[index][0])/float(scene_stamps[tup[0]][1]-scene_stamps[tup[0]][0])) > ((sub_stamps[index][1]-scene_stamps[tup[1]][0])/float(scene_stamps[tup[1]][1]-scene_stamps[tup[1]][0])):\n",
    "                sub_to_shot[index] = tup[0]\n",
    "            else:\n",
    "                sub_to_shot[index] = tup[1]\n",
    "    return sub_to_shot\n",
    "\n",
    "def plot_shot_assigner(plot_to_sub, sub_to_shot):\n",
    "    ''' plot_to_sub[i] gives the matching list of subtitle sentences ->\n",
    "        [(35, 0.2656571164563915), (604, 0.2658152134299805), (619, 0.26629063540377135),\n",
    "         (624, 0.44261725639867383), (687, 0.3904935983047358)] '''\n",
    "    temp, plot_to_shot = [[] for i in range(len(plot_to_sub))], [[] for i in range(len(plot_to_sub))]  # as many as the number of plot sentences\n",
    "    for i in range(len(plot_to_sub)):\n",
    "        temp[i] = [sub_to_shot[j[0]] for j in plot_to_sub[i]]\n",
    "    # use this method instead of list(set())\n",
    "    for i in range(len(plot_to_sub)):\n",
    "        for item in temp[i]:\n",
    "            if item not in plot_to_shot[i]:\n",
    "                plot_to_shot[i].append(item)\n",
    "    # now plot_to_shot has the sorted list of shots\n",
    "    return plot_to_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# above line required to make non ascii characters work\n",
    "# ffmpeg -i test.mp4 -vf select='gt(scene\\,0.4)' -vsync vfr thumb%04d.png\n",
    "# use above to get thumbnail output\n",
    "import os, sys, re\n",
    "import pdb\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize    # nltk sentence and word tokenizers\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()                  # initialization of the lemmatizer\n",
    "from nltk.corpus import stopwords                         # get list of stop words\n",
    "import subprocess, shlex\n",
    "symbols_list = ['...', \"''\",  '.', '!', '?', ',', '``', '--', '[', ']', '<', '>', '♪', '/i', '/', ';', '(', ')', '-', ':', '¡', '¿']\n",
    "sc = SparkContext('local[*]', 'PySpark')\n",
    "\n",
    "# tokenizer functionality common to all functions\n",
    "def tokenizer(query):\n",
    "    temp = word_tokenize(query.lower()) # needs to be lowered for sim. 2\n",
    "    temp = [j for j in temp if j not in symbols_list]\n",
    "    temp = [wordnet_lemmatizer.lemmatize(j) for j in temp]\n",
    "    temp = [j for j in temp if j not in stopwords.words('english')]\n",
    "    return temp    # return a pointer to the list\n",
    "\n",
    "class VideoFile(object):\n",
    "    def __init__(self, path_to_srt, path_to_plot):\n",
    "        global sc\n",
    "        self.plot_sentences = self.extract_plot_sentences(path_to_plot)\n",
    "        self.stamp_data = self.get_stamps()\n",
    "        self.subtitle_data = self.get_subtitle_data(path_to_srt)\n",
    "        plot_shot_data = plot_shot_helper(self.plot_sentences, self.subtitle_data,\n",
    "                                          self.stamp_data)\n",
    "        self.plot_to_shot = plot_shot_data['plot_to_shot']\n",
    "        self.idf = plot_shot_data['idf']\n",
    "        self.tfIdf = plot_shot_data['tfIdf']\n",
    "        self.tf_list = plot_shot_data['tf_list']\n",
    "        print('Processing complete')\n",
    "        sys.stderr.write('\\nProcessing complete\\n')\n",
    "        sc.stop()\n",
    "\n",
    "    def extract_plot_sentences(self, plot_txt_path):\n",
    "        sys.stderr.write('\\nStarting plot sentence processing ...\\n')\n",
    "        plot_file = sc.textFile(plot_txt_path)\n",
    "        plot_sentences = plot_file.flatMap(sent_tokenize).coalesce(1).collect()\n",
    "        plot_sentences = [tokenizer(i) for i in plot_sentences]\n",
    "        return plot_sentences\n",
    "\n",
    "    # preprocessing to get the scenes detected in the video\n",
    "    # scene stamps define the scene boundaries\n",
    "    def get_stamps(self):\n",
    "        time_stamps, scene_stamps = [0], []\n",
    "        sys.stderr.write('\\nSubtitles processing started ...')\n",
    "        subtitles_data = sc.textFile('hdfs://localhost:9000/user/arun/scenes.csv').zipWithIndex().filter(lambda x: x[1] > 1).map(lambda line: line[0].split(',')).glom().collect()\n",
    "        for ts in subtitles_data[0]:\n",
    "            time_stamps.append(float(ts[3]))  # the timestamp is the third field\n",
    "            scene_stamps.append((time_stamps[-2], time_stamps[-1]))\n",
    "        return {'time_stamps': time_stamps, 'scene_stamps': scene_stamps}\n",
    "\n",
    "    def get_subtitle_data(self, sub_file_path):\n",
    "        sub_stamps, sub_text, buf = [], [], []\n",
    "        subs_rdd = sc.newAPIHadoopFile(sub_file_path,\n",
    "                                       \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\",\n",
    "                                       \"org.apache.hadoop.io.LongWritable\",\n",
    "                                       \"org.apache.hadoop.io.Text\",\n",
    "            conf={\"textinputformat.record.delimiter\": '\\n'}).map(lambda line: line[1]).collect()\n",
    "        # subs = sc.textFile(sub_file_path).map(lambda line: line).collect()\n",
    "        print('Processing subs ...')\n",
    "        # with open(sub_file_path) as fp:\n",
    "        #     subs = fp.readlines()\n",
    "        for index, line in enumerate(subs_rdd):\n",
    "            l = line.strip()\n",
    "            if l:\n",
    "                buf.append(l)\n",
    "            if (not l or index == len(subs_rdd) - 1):\n",
    "                # first process the time stamps\n",
    "                temp = re.split(' --> ', buf[1])\n",
    "                temp_1, temp_2 = temp[0].split(':'), temp[1].split(':')\n",
    "                # convert the time stamp into seconds only...\n",
    "                temp_1 = float(temp_1[1])*60 + float(''.join(temp_1[2].split(',')))/1000.0\n",
    "                temp_2 = float(temp_2[1])*60 + float(''.join(temp_2[2].split(',')))/1000.0\n",
    "                sub_stamps.append((temp_1, temp_2))\n",
    "                sub_text.append(' '.join(buf[2:]))\n",
    "                buf = []\n",
    "        raw_sub_text = sub_text\n",
    "        temp = []\n",
    "        for sub in sub_text:\n",
    "            temp.append(tokenizer(sub))\n",
    "        sub_text = temp\n",
    "        return {'sub_stamps': sub_stamps, 'sub_text': sub_text, 'raw_sub_text': raw_sub_text}\n",
    "\n",
    "    def search_shot(self, query):\n",
    "        processed_query = tokenizer(query)            # tokenizer accepts a list\n",
    "        max_sim, max_sim_sentence = cosine_similarity(processed_query,\n",
    "                                                      self.idf, self.tfIdf,\n",
    "                                                      len(self.plot_sentences))\n",
    "        sys.stderr.write('\\nSearch shot started\\n')\n",
    "        print('\\nIdf is', self.idf)\n",
    "        if max_sim_sentence == 'None':\n",
    "            sys.stderr.write('\\nMax is none\\n')\n",
    "            return {'shot_timestamps': -1, 'max_sim': -1}\n",
    "        sys.stderr.write('\\nSearch is valid\\n')\n",
    "        shots_list = self.plot_to_shot[max_sim_sentence]\n",
    "        shot_timestamps = [self.stamp_data['time_stamps'][shot] for shot in shots_list]\n",
    "        return {'shot_timestamps': shot_timestamps, 'max_sim': max_sim}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video = VideoFile('hdfs://localhost:9000/user/arun/metadata/testvideo.srt', 'hdfs://localhost:9000/user/arun/metadata/plot.txt')\n",
    "    query = raw_input('Enter a search query : ')\n",
    "    search_results = video.search_shot(query)\n",
    "    print(search_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
